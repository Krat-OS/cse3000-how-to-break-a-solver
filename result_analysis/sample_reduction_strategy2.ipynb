{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from result_analysis.helper_functions import (\n",
    "  process_csv_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78c7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_original_format = pd.read_csv(\"/home/csoare/experiments/feature_analysis3/features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4982c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = process_csv_data(\"/home/csoare/experiments/feature_analysis3/features.csv\")\n",
    "\n",
    "# Total instances\n",
    "total_instances = len(features)\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Total number of instances: {total_instances}\")\n",
    "# Print number of instances per generator\n",
    "instances_per_generator = features['generator'].value_counts()\n",
    "print(\"Number of instances per generator:\")\n",
    "print(instances_per_generator)\n",
    "\n",
    "def find_missing_instances(cnf_dir: str, features_csv: str) -> list:\n",
    "    \"\"\"\n",
    "    Find CNF files that were not processed in the features CSV.\n",
    "    \n",
    "    Args:\n",
    "        cnf_dir (str): Directory containing the original CNF files\n",
    "        features_csv (str): Path to the features CSV file\n",
    "        \n",
    "    Returns:\n",
    "        list: Names of CNF files that were not processed\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Read all CNF filenames\n",
    "    cnf_files = set()\n",
    "    for cnf_file in Path(cnf_dir).glob(\"*.cnf\"):\n",
    "        cnf_files.add(cnf_file.stem)\n",
    "    \n",
    "    # Read processed instances from CSV\n",
    "    df = pd.read_csv(features_csv)\n",
    "    processed_instances = set(df['instance_name'])\n",
    "    \n",
    "    # Find missing instances\n",
    "    missing = cnf_files - processed_instances\n",
    "    \n",
    "    return sorted(list(missing))\n",
    "\n",
    "missing = find_missing_instances(\"/home/csoare/experiments/feature_analysis3/instances/cnf\", \"/home/csoare/experiments/feature_analysis3/features.csv\")\n",
    "if missing:\n",
    "    print(f\"Found {len(missing)} unprocessed instances:\")\n",
    "    for instance in missing:\n",
    "        print(f\"  - {instance}\")\n",
    "else:\n",
    "    print(\"All CNF files were processed successfully\")\n",
    "\n",
    "print(features.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new columns\n",
    "print(\"\\nUnique presumed difficulties:\", features['presumed_difficulty'].unique())\n",
    "print(\"Unique randomness values:\", features['randomness'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b8195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for all analyses\n",
    "def get_satzilla_features(df):\n",
    "    \"\"\"Get numeric features, excluding metadata columns\"\"\"\n",
    "    features_to_exclude = ['seed', 'randomness', 'solved']\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    return [col for col in numeric_cols if col not in features_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848c76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_distribution_preserving_instances(satzilla_features, instances_per_generator=100):\n",
    "    \"\"\"\n",
    "    Select instances while preserving the original distribution of features.\n",
    "    Ensures exactly instances_per_generator instances are selected for each generator.\n",
    "    \n",
    "    Args:\n",
    "        satzilla_features (pd.DataFrame): Original dataset with features\n",
    "        instances_per_generator (int): Number of instances to select per generator\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Selected instances maintaining original distribution\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for generator in satzilla_features['generator'].unique():\n",
    "        generator_instances = satzilla_features[satzilla_features['generator'] == generator].copy()\n",
    "        \n",
    "        # If we have fewer instances than requested, take all of them\n",
    "        if len(generator_instances) <= instances_per_generator:\n",
    "            # Randomly duplicate instances if we have fewer than required\n",
    "            if len(generator_instances) < instances_per_generator:\n",
    "                n_duplicates = instances_per_generator - len(generator_instances)\n",
    "                duplicate_indices = np.random.choice(len(generator_instances), size=n_duplicates, replace=True)\n",
    "                duplicates = generator_instances.iloc[duplicate_indices]\n",
    "                generator_instances = pd.concat([generator_instances, duplicates])\n",
    "            results.append(generator_instances)\n",
    "            continue\n",
    "            \n",
    "        feature_columns = get_satzilla_features(generator_instances)\n",
    "        features = generator_instances[feature_columns]\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "        \n",
    "        # Create bins for each feature to stratify the sampling\n",
    "        n_bins = min(10, len(generator_instances) // 20)  # Adaptive bin size\n",
    "        bin_labels = []\n",
    "        \n",
    "        for i in range(scaled_features.shape[1]):\n",
    "            bins = pd.qcut(scaled_features[:, i], q=n_bins, labels=False, duplicates='drop')\n",
    "            bin_labels.append(bins)\n",
    "            \n",
    "        # Combine bin labels to create strata\n",
    "        combined_bins = np.vstack(bin_labels).T\n",
    "        strata = ['_'.join(map(str, row)) for row in combined_bins]\n",
    "        \n",
    "        # Calculate target size for each stratum\n",
    "        stratum_counts = pd.Series(strata).value_counts()\n",
    "        total_samples = len(strata)\n",
    "        target_sizes = {}\n",
    "        \n",
    "        remaining_instances = instances_per_generator\n",
    "        \n",
    "        # First pass: allocate minimum sizes\n",
    "        for stratum in stratum_counts.index:\n",
    "            proportion = stratum_counts[stratum] / total_samples\n",
    "            target_size = max(1, int(instances_per_generator * proportion))\n",
    "            # Make sure we don't exceed remaining instances\n",
    "            target_size = min(target_size, remaining_instances)\n",
    "            target_sizes[stratum] = target_size\n",
    "            remaining_instances -= target_size\n",
    "            \n",
    "        # Second pass: distribute any remaining instances\n",
    "        if remaining_instances > 0:\n",
    "            strata_by_size = sorted(stratum_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            for stratum, _ in strata_by_size:\n",
    "                if remaining_instances <= 0:\n",
    "                    break\n",
    "                target_sizes[stratum] += 1\n",
    "                remaining_instances -= 1\n",
    "                \n",
    "        # Select instances from each stratum\n",
    "        selected_indices = []\n",
    "        for stratum in target_sizes:\n",
    "            if target_sizes[stratum] == 0:\n",
    "                continue\n",
    "                \n",
    "            stratum_indices = np.where(np.array(strata) == stratum)[0]\n",
    "            if len(stratum_indices) > 0:\n",
    "                # Handle case where we need more samples than available\n",
    "                size = target_sizes[stratum]\n",
    "                if size > len(stratum_indices):\n",
    "                    # Take all available indices\n",
    "                    selected_indices.extend(stratum_indices)\n",
    "                    # Randomly sample the remaining with replacement\n",
    "                    additional_samples = np.random.choice(\n",
    "                        stratum_indices,\n",
    "                        size=size - len(stratum_indices),\n",
    "                        replace=True\n",
    "                    )\n",
    "                    selected_indices.extend(additional_samples)\n",
    "                else:\n",
    "                    # Normal case: sample without replacement\n",
    "                    selected_from_stratum = np.random.choice(\n",
    "                        stratum_indices,\n",
    "                        size=size,\n",
    "                        replace=False\n",
    "                    )\n",
    "                    selected_indices.extend(selected_from_stratum)\n",
    "        \n",
    "        selected_instances = generator_instances.iloc[selected_indices]\n",
    "        \n",
    "        # Double-check we have exactly the right number of instances\n",
    "        if len(selected_instances) < instances_per_generator:\n",
    "            n_missing = instances_per_generator - len(selected_instances)\n",
    "            additional_samples = generator_instances.sample(n=n_missing, replace=True)\n",
    "            selected_instances = pd.concat([selected_instances, additional_samples])\n",
    "        elif len(selected_instances) > instances_per_generator:\n",
    "            selected_instances = selected_instances.sample(n=instances_per_generator, replace=False)\n",
    "            \n",
    "        results.append(selected_instances)\n",
    "        \n",
    "    return pd.concat(results, axis=0)\n",
    "\n",
    "selected_dataset = select_distribution_preserving_instances(features, instances_per_generator=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap('viridis')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "def plot_feature_distributions(original_df, selected_df, feature_cols):\n",
    "    \"\"\"Plot PDF comparisons for features before and after selection for each generator.\"\"\"\n",
    "    plt.set_cmap('viridis')\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    for generator in original_df['generator'].unique():\n",
    "        n_rows = (len(feature_cols) + 2) // 3\n",
    "        fig = plt.figure(figsize=(15, 5*n_rows + 1))  # Added extra space for legend\n",
    "        \n",
    "        # Create a gridspec with space for the legend\n",
    "        gs = plt.GridSpec(n_rows + 1, 3, height_ratios=[0.1] + [1]*n_rows)\n",
    "        \n",
    "        # Create figure-level legend handles\n",
    "        legend_handles = []\n",
    "        legend_labels = []\n",
    "        \n",
    "        fig.suptitle(f'Feature Distributions for {generator}', fontsize=16, y=0.98)\n",
    "        \n",
    "        orig_data = original_df[original_df['generator'] == generator]\n",
    "        sel_data = selected_df[selected_df['generator'] == generator]\n",
    "        \n",
    "        orig_color = plt.cm.viridis(0.2)\n",
    "        sel_color = plt.cm.viridis(0.8)\n",
    "        \n",
    "        for idx, feature in enumerate(feature_cols):\n",
    "            row = idx // 3 + 1  # +1 because first row is for legend\n",
    "            col = idx % 3\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            \n",
    "            orig_feature = orig_data[feature].values\n",
    "            sel_feature = sel_data[feature].values\n",
    "            \n",
    "            # Define common x-range for both distributions\n",
    "            x_min = min(np.min(orig_feature), np.min(sel_feature))\n",
    "            x_max = max(np.max(orig_feature), np.max(sel_feature))\n",
    "            \n",
    "            range_width = x_max - x_min\n",
    "            x_min -= range_width * 0.05 if range_width > 0 else 0.5\n",
    "            x_max += range_width * 0.05 if range_width > 0 else 0.5\n",
    "            \n",
    "            xs = np.linspace(x_min, x_max, 200)\n",
    "            \n",
    "            # Plot distributions\n",
    "            is_orig_constant = orig_data[feature].nunique() == 1\n",
    "            is_sel_constant = sel_data[feature].nunique() == 1\n",
    "            \n",
    "            # Original distribution\n",
    "            if is_orig_constant:\n",
    "                value = orig_feature[0]\n",
    "                line1 = ax.axvline(x=value, color=orig_color, \n",
    "                                 label='Full dataset (1000 instances)', \n",
    "                                 linestyle=':', linewidth=2, alpha=0.8)\n",
    "            else:\n",
    "                try:\n",
    "                    if orig_feature.std() < 1e-6:\n",
    "                        orig_feature = orig_feature + np.random.normal(0, 1e-6, len(orig_feature))\n",
    "                    \n",
    "                    density = gaussian_kde(orig_feature, bw_method='silverman')\n",
    "                    ys = density(xs)\n",
    "                    ys = ys / (np.max(ys) + 1e-10)\n",
    "                    line1 = ax.plot(xs, ys, color=orig_color, linewidth=2,\n",
    "                                  label='Full dataset (1000 instances)')[0]\n",
    "                    ax.fill_between(xs, ys, alpha=0.2, color=orig_color)\n",
    "                except Exception:\n",
    "                    # Fallback to histogram for numerical issues\n",
    "                    counts, bins, patches = ax.hist(orig_feature, bins=30, density=True,\n",
    "                                                  alpha=0.3, color=orig_color,\n",
    "                                                  label='Full dataset (1000 instances)')\n",
    "                    line1 = ax.axvline(np.median(orig_feature), color=orig_color,\n",
    "                                     linestyle='--')\n",
    "            \n",
    "            # Selected distribution\n",
    "            if is_sel_constant:\n",
    "                value = sel_feature[0]\n",
    "                line2 = ax.axvline(x=value, color=sel_color,\n",
    "                                 label='Selected subset (100 instances)', \n",
    "                                 linestyle=':', linewidth=2, alpha=0.8)\n",
    "            else:\n",
    "                try:\n",
    "                    if sel_feature.std() < 1e-6:\n",
    "                        sel_feature = sel_feature + np.random.normal(0, 1e-6, len(sel_feature))\n",
    "                    \n",
    "                    density = gaussian_kde(sel_feature, bw_method='silverman')\n",
    "                    ys = density(xs)\n",
    "                    ys = ys / (np.max(ys) + 1e-10)\n",
    "                    line2 = ax.plot(xs, ys, color=sel_color, linewidth=2,\n",
    "                                  label='Selected subset (100 instances)')[0]\n",
    "                    ax.fill_between(xs, ys, alpha=0.2, color=sel_color)\n",
    "                except Exception:\n",
    "                    counts, bins, patches = ax.hist(sel_feature, bins=30, density=True,\n",
    "                                                  alpha=0.3, color=sel_color,\n",
    "                                                  label='Selected subset (100 instances)')\n",
    "                    line2 = ax.axvline(np.median(sel_feature), color=sel_color,\n",
    "                                     linestyle='--')\n",
    "            \n",
    "            if idx == 0:  # Only add to legend handles once\n",
    "                legend_handles.extend([line1, line2])\n",
    "                legend_labels.extend(['Full dataset (1000 instances)', 'Selected subset (100 instances)'])\n",
    "            \n",
    "            # Calculate statistics\n",
    "            orig_std = orig_feature.std()\n",
    "            sel_std = sel_feature.std()\n",
    "            if orig_std == 0:\n",
    "                stats_text = \"Constant value in both datasets\"\n",
    "                title_text = f\"{feature}\\n(Constant feature)\"\n",
    "            else:\n",
    "                diff_pct = ((sel_std / orig_std) - 1) * 100\n",
    "                stats_text = (f'Full dataset Std Dev: {orig_std:.2f}\\n'\n",
    "                            f'Selected subset Std Dev: {sel_std:.2f}')\n",
    "                title_text = f'{feature}\\nStd Dev change: {diff_pct:+.1f}%'\n",
    "            \n",
    "            ax.set_title(title_text, fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "            \n",
    "            ax.text(0.02, 0.98, stats_text,\n",
    "                   transform=ax.transAxes,\n",
    "                   verticalalignment='top',\n",
    "                   fontsize=12)\n",
    "        \n",
    "        # Add the legend in the reserved space at the top\n",
    "        fig.legend(legend_handles, legend_labels,\n",
    "                  loc='upper center',\n",
    "                  bbox_to_anchor=(0.5, 0.95),\n",
    "                  ncol=2,\n",
    "                  fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "\n",
    "selected_features = ['VCG-CLAUSE-mean', 'POSNEG-RATIO-VAR-mean', 'CG-mean']\n",
    "plot_feature_distributions(features, selected_dataset, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_selected_instances(satzilla_features_original, selected_dataset, source_cnf_dir, target_base_dir):\n",
    "    \"\"\"\n",
    "    Organize selected CNF files into folders based on base generator and difficulty.\n",
    "    Only processes instances from selected_dataset but maintains features format from original.\n",
    "    \n",
    "    Args:\n",
    "        satzilla_features_original (pd.DataFrame): Original DataFrame containing all instance features\n",
    "        selected_dataset (pd.DataFrame): DataFrame containing the selected instances\n",
    "        source_cnf_dir (str): Directory containing source CNF files\n",
    "        target_base_dir (str): Target directory for organized files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics about copied files\n",
    "    \"\"\"\n",
    "    target_base_path = Path(target_base_dir)\n",
    "    target_base_path.mkdir(parents=True, exist_ok=True)\n",
    "    source_path = Path(source_cnf_dir)\n",
    "    \n",
    "    stats = {\n",
    "        'total_copied': 0,\n",
    "        'files_per_group': {},\n",
    "        'successfully_copied': []  # Track which instances were actually copied\n",
    "    }\n",
    "    \n",
    "    # Parse generator and difficulty from instance names\n",
    "    def get_generator_info(instance_name):\n",
    "        parts = instance_name.split('_')[0].split('-')\n",
    "        if len(parts) >= 3:\n",
    "            base_generator = parts[0]\n",
    "            difficulty = parts[1]\n",
    "            return base_generator, difficulty\n",
    "        return None, None\n",
    "    \n",
    "    # Get list of selected instance names\n",
    "    selected_instances = selected_dataset.index.tolist()\n",
    "    \n",
    "    # Group instances by base generator and difficulty\n",
    "    instance_groups = {}\n",
    "    for instance in selected_instances:\n",
    "        base_generator, difficulty = get_generator_info(instance)\n",
    "        if base_generator and difficulty:\n",
    "            group_key = f\"{base_generator}-{difficulty}\"\n",
    "            if group_key not in instance_groups:\n",
    "                instance_groups[group_key] = []\n",
    "            instance_groups[group_key].append(instance)\n",
    "    \n",
    "    # Copy files to appropriate directories\n",
    "    for group_dir_name, instances in instance_groups.items():\n",
    "        group_dir = target_base_path / group_dir_name\n",
    "        group_dir.mkdir(exist_ok=True)\n",
    "        files_copied = 0\n",
    "        \n",
    "        for instance in instances:\n",
    "            source_file = source_path / f\"{instance}.cnf\"\n",
    "            target_file = group_dir / f\"{instance}.cnf\"\n",
    "            \n",
    "            try:\n",
    "                if source_file.exists():\n",
    "                    shutil.copy2(source_file, target_file)\n",
    "                    files_copied += 1\n",
    "                    stats['successfully_copied'].append(instance)\n",
    "                else:\n",
    "                    print(f\"Warning: Source file not found: {source_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {instance}: {str(e)}\")\n",
    "        \n",
    "        stats['files_per_group'][group_dir_name] = files_copied\n",
    "        stats['total_copied'] += files_copied\n",
    "    \n",
    "    # Save features only for successfully copied instances\n",
    "    if stats['successfully_copied']:\n",
    "        # Filter features for only successfully copied instances from the original format\n",
    "        features_output = satzilla_features_original[\n",
    "            satzilla_features_original['instance_name'].isin(stats['successfully_copied'])\n",
    "        ]\n",
    "        \n",
    "        # Save to CSV in target directory\n",
    "        features_output_path = target_base_path / 'features_output.csv'\n",
    "        features_output.to_csv(features_output_path, index=False)  # Don't include index since instance_name is a column\n",
    "        print(f\"\\nFeatures saved to: {features_output_path}\")\n",
    "        \n",
    "        # Add to stats\n",
    "        stats['features_file'] = str(features_output_path)\n",
    "        stats['features_count'] = len(features_output)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Usage example:\n",
    "source_dir = \"/home/csoare/experiments/feature_analysis3/instances/cnf\"\n",
    "target_dir = \"/home/csoare/experiments/reduced_instances_FINAL_MASIV_LASCONI\"\n",
    "stats = organize_selected_instances(features_original_format, selected_dataset, source_dir, target_dir)\n",
    "\n",
    "print(\"\\nCopying complete!\")\n",
    "print(f\"Total files copied: {stats['total_copied']}\")\n",
    "print(f\"Features saved for {stats['features_count']} instances\")\n",
    "print(\"\\nFiles copied per group:\")\n",
    "for group, count in stats['files_per_group'].items():\n",
    "    print(f\"{group}: {count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_instances_and_save(satzilla_features: pd.DataFrame, seed: int, randomness: int = 100, output_file: str = 'selected_instances.txt') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select one random instance from each base generator + difficulty combination,\n",
    "    only considering instances with specified randomness, and save to text file.\n",
    "    \n",
    "    Args:\n",
    "        satzilla_features (pd.DataFrame): DataFrame containing instance features\n",
    "        seed (int): Random seed for reproducibility\n",
    "        randomness (int): Randomness value to filter instances\n",
    "        output_file (str): Path to the output text file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the selected instances\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if satzilla_features.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "        \n",
    "    # Verify required columns exist\n",
    "    required_cols = ['base_generator', 'presumed_difficulty', 'randomness']\n",
    "    missing_cols = [col for col in required_cols if col not in satzilla_features.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Convert randomness to string to ensure matching works\n",
    "    randomness = str(randomness)\n",
    "    \n",
    "    results = []\n",
    "    # Print unique values to debug\n",
    "    print(f\"Unique base generators: {satzilla_features['base_generator'].unique()}\")\n",
    "    print(f\"Unique difficulties: {satzilla_features['presumed_difficulty'].unique()}\")\n",
    "    print(f\"Unique randomness values: {satzilla_features['randomness'].unique()}\")\n",
    "    \n",
    "    for base in sorted(satzilla_features['base_generator'].unique()):\n",
    "        for diff in sorted(satzilla_features['presumed_difficulty'].unique()):\n",
    "            mask = (satzilla_features['base_generator'] == base) & \\\n",
    "                   (satzilla_features['presumed_difficulty'] == diff) & \\\n",
    "                   (satzilla_features['randomness'] == randomness)\n",
    "            \n",
    "            generator_instances = satzilla_features[mask]\n",
    "            \n",
    "            print(f\"Found {len(generator_instances)} instances for {base}-{diff} with randomness {randomness}\")\n",
    "            \n",
    "            if not generator_instances.empty:\n",
    "                random_instance = generator_instances.sample(n=1, random_state=seed)\n",
    "                results.append(random_instance)\n",
    "    \n",
    "    # Check if any instances were found\n",
    "    if not results:\n",
    "        raise ValueError(f\"No instances found with randomness={randomness}\")\n",
    "    \n",
    "    final_dataset = pd.concat(results, axis=0)\n",
    "    \n",
    "    # Create output text file\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write instances\n",
    "        for base in sorted(final_dataset['base_generator'].unique()):\n",
    "            for diff in sorted(final_dataset['presumed_difficulty'].unique()):\n",
    "                mask = (final_dataset['base_generator'] == base) & \\\n",
    "                       (final_dataset['presumed_difficulty'] == diff)\n",
    "                if mask.any():\n",
    "                    instance = final_dataset[mask].index[0]\n",
    "                    f.write(f\"{base}-{diff}: {instance}\\n\")\n",
    "        # Write current directory path as last line\n",
    "        f.write(f\"\\nCurrent working directory:\\n{os.getcwd()}\\n\")\n",
    "    \n",
    "    print(f\"\\nSelected {len(final_dataset)} instances, saved to {output_file}\")\n",
    "    return final_dataset\n",
    "\n",
    "random_dataset = select_random_instances_and_save(selected_dataset, seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
