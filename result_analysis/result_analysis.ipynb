{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:16.434201Z",
     "start_time": "2025-01-11T14:52:15.700989Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from result_analysis.helper_functions import (\n",
    "  process_csv_data,\n",
    "  merge_data,\n",
    "  classify_fuzzing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:16.641344Z",
     "start_time": "2025-01-11T14:52:16.498314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "experiment1_results = process_csv_data(\"/home/csoare/experiment1/fuzz_results_with_cpog.csv\")\n",
    "experiment1_features = process_csv_data(\"/home/csoare/experiment1/features_output.csv\")\n",
    "\n",
    "white_cmap = sns.light_palette(\"white\", as_cmap=True)\n",
    "\n",
    "\n",
    "merged_data = merge_data(experiment1_results, experiment1_features)\n",
    "\n",
    "print(merged_data.columns)\n",
    "print(merged_data[\"counter\"].unique())\n",
    "print(merged_data[\"generator\"].unique())\n",
    "print(merged_data[\"cpog_message\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:17.880869Z",
     "start_time": "2025-01-11T14:52:17.160814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate into subsets\n",
    "timeout_results, crash_results, correct_results, incorrect_results, *cpog_subsets = classify_fuzzing_results(merged_data)\n",
    "\n",
    "merged_data[\"crashed\"] = False\n",
    "if 'crash_results' in locals():\n",
    "    merged_data.loc[merged_data.index.isin(crash_results.index), \"crashed\"] = True\n",
    "\n",
    "# Build (original_label, df) list\n",
    "subset_label_pairs = [\n",
    "    (\"timeout_results\", timeout_results),\n",
    "    (\"crash_results\", crash_results),\n",
    "    (\"correct_results\", correct_results),\n",
    "    (\"incorrect_results\", incorrect_results),\n",
    "]\n",
    "for i, cpog_df in enumerate(cpog_subsets, start=1):\n",
    "    if not cpog_df.empty:\n",
    "        msg = cpog_df[\"cpog_message\"].iloc[0]\n",
    "    else:\n",
    "        msg = f\"CPOG_Message_{i}\"\n",
    "    subset_label_pairs.append((f\"CPOG: {msg}\", cpog_df))\n",
    "\n",
    "# Map original labels to final labels\n",
    "def map_label(original_label: str) -> str:\n",
    "    if original_label == \"timeout_results\":\n",
    "        return \"timeout\"\n",
    "    elif original_label == \"crash_results\":\n",
    "        return \"crash\"\n",
    "    elif original_label == \"correct_results\":\n",
    "        return \"correct count\"\n",
    "    elif original_label == \"incorrect_results\":\n",
    "        return \"incorrect count\"\n",
    "    elif original_label.startswith(\"CPOG: \"):\n",
    "        # Remove \"CPOG: \" and any \":\" character\n",
    "        return original_label.replace(\"CPOG: \", \"\").replace(\":\", \"\")\n",
    "    else:\n",
    "        return original_label\n",
    "\n",
    "# Gather all data into a list of (final_label, counter_val, count_val), grouping CPOG errors\n",
    "all_data_grouped = []\n",
    "for orig_label, df in subset_label_pairs:\n",
    "    if \"counter\" not in df.columns:\n",
    "        continue\n",
    "    if orig_label.startswith(\"CPOG: \"):\n",
    "        final_label = \"CPOG errors\"  # Group all CPOG-related data under this label\n",
    "    else:\n",
    "        final_label = map_label(orig_label)\n",
    "    # Count how many rows for each 'counter' value\n",
    "    counts_series = df.groupby(\"counter\").size() if not df.empty else pd.Series()\n",
    "    for counter_val, count_val in counts_series.items():\n",
    "        all_data_grouped.append((final_label, counter_val, count_val))\n",
    "\n",
    "# Aggregate duplicate entries by summing counts\n",
    "counts_grouped_df = (\n",
    "    pd.DataFrame(all_data_grouped, columns=[\"subset_label\", \"counter_val\", \"count_val\"])\n",
    "    .groupby([\"subset_label\", \"counter_val\"], as_index=False)\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "pivot_grouped_df = (\n",
    "    counts_grouped_df\n",
    "    .pivot(index=\"counter_val\", columns=\"subset_label\", values=\"count_val\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Ensure all columns appear, even if empty\n",
    "desired_col_order_grouped = [\n",
    "    \"correct count\",\n",
    "    \"incorrect count\",\n",
    "    \"crash\",\n",
    "    \"timeout\",\n",
    "    \"CPOG errors\"\n",
    "]\n",
    "pivot_grouped_df = pivot_grouped_df.reindex(columns=desired_col_order_grouped, fill_value=0)\n",
    "\n",
    "\n",
    "# Plot the grouped version\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_grouped_df.T,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=white_cmap,\n",
    "    cbar=False,\n",
    "    linecolor=\"black\",\n",
    "    linewidths=1.0,\n",
    "    annot_kws={\"size\": 18},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Distribution of Instance Types by Counter\", fontsize=16, pad=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_ylabel(\"Instance Type\", fontsize=14)\n",
    "ax.set_xlabel(\"Counter Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For detailed CPOG errors\n",
    "cpog_data = []\n",
    "for orig_label, df in subset_label_pairs:\n",
    "    if orig_label.startswith(\"CPOG: \") and \"counter\" in df.columns:\n",
    "        final_label = map_label(orig_label)\n",
    "        counts_series = df.groupby(\"counter\").size() if not df.empty else pd.Series()\n",
    "        for counter_val, count_val in counts_series.items():\n",
    "            cpog_data.append((final_label, counter_val, count_val))\n",
    "\n",
    "if cpog_data:  # Only create second plot if there are CPOG errors\n",
    "    cpog_df = (\n",
    "        pd.DataFrame(cpog_data, columns=[\"subset_label\", \"counter_val\", \"count_val\"])\n",
    "        .groupby([\"subset_label\", \"counter_val\"], as_index=False)\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    pivot_cpog_df = (\n",
    "        cpog_df\n",
    "        .pivot(index=\"counter_val\", columns=\"subset_label\", values=\"count_val\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        pivot_cpog_df.T,\n",
    "        annot=True,\n",
    "        fmt=\"g\",\n",
    "        cmap=white_cmap,\n",
    "        cbar=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidths=1.0,\n",
    "        annot_kws={\"size\": 18},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(\"Distribution of CPOG Errors by Message and Counter\", fontsize=16, pad=20)\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.set_xlabel(\"Counter Name\", fontsize=14)\n",
    "    ax.set_ylabel(\"CPOG Message\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:18.781614Z",
     "start_time": "2025-01-11T14:52:18.151934Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_grouped = []\n",
    "unique_counters = merged_data[\"counter\"].nunique()\n",
    "for orig_label, df in subset_label_pairs:\n",
    "  if \"generator\" not in df.columns or \"counter\" not in df.columns:\n",
    "    continue\n",
    "  if orig_label.startswith(\"CPOG: \"):\n",
    "    final_label = \"CPOG errors\"\n",
    "  else:\n",
    "    final_label = map_label(orig_label)\n",
    "  counts_series = df.groupby(\"generator\").size().apply(lambda x: int(x / unique_counters))\n",
    "  for generator_val, count_val in counts_series.items():\n",
    "    all_data_grouped.append((final_label, generator_val, count_val))\n",
    "\n",
    "counts_grouped_df = (\n",
    "  pd.DataFrame(all_data_grouped, columns=[\"subset_label\", \"generator_val\", \"count_val\"])\n",
    "  .groupby([\"subset_label\", \"generator_val\"], as_index=False)\n",
    "  .sum()\n",
    ")\n",
    "\n",
    "pivot_grouped_df = (\n",
    "  counts_grouped_df\n",
    "  .pivot(index=\"generator_val\", columns=\"subset_label\", values=\"count_val\")\n",
    "  .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "ordered_generators = [\n",
    "  'FuzzSAT-easy',\n",
    "  'FuzzSAT-medium',\n",
    "  'FuzzSAT-hard',\n",
    "  'FuzzSAT-mixed',\n",
    "  'FuzzSAT-random-medium',\n",
    "  'FuzzSAT-random-hard',\n",
    "  'FuzzSAT-structured-hard'\n",
    "]\n",
    "pivot_grouped_df = pivot_grouped_df.reindex(columns=desired_col_order_grouped, index=ordered_generators, fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "  pivot_grouped_df.T,\n",
    "  fmt=\"g\",\n",
    "  annot=True,\n",
    "  cmap=white_cmap,\n",
    "  cbar=False,\n",
    "  linecolor=\"black\",\n",
    "  linewidths=1.0,\n",
    "  annot_kws={\"size\": 20},\n",
    "  ax=ax\n",
    ")\n",
    "ax.set_title(\"Distribution of Instance Types by Generator (Grouped CPOG Errors)\", fontsize=14, pad=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.set_ylabel(\"Instance Type\", fontsize=12)\n",
    "ax.set_xlabel(\"Generator Name\", fontsize=12)\n",
    "plt.xticks(rotation=30, fontsize=10, ha='right')\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cpog_data = []\n",
    "for orig_label, df in subset_label_pairs:\n",
    "  if orig_label.startswith(\"CPOG: \") and \"generator\" in df.columns and \"counter\" in df.columns:\n",
    "    final_label = map_label(orig_label)\n",
    "    unique_counters = df[\"counter\"].nunique()\n",
    "    counts_series = df.groupby(\"generator\").size().apply(lambda x: int(math.ceil(x / unique_counters)))\n",
    "    for generator_val, count_val in counts_series.items():\n",
    "      cpog_data.append((final_label, generator_val, count_val))\n",
    "\n",
    "cpog_df = (\n",
    "  pd.DataFrame(cpog_data, columns=[\"subset_label\", \"generator_val\", \"count_val\"])\n",
    "  .groupby([\"subset_label\", \"generator_val\"], as_index=False)\n",
    "  .sum()\n",
    ")\n",
    "\n",
    "pivot_cpog_df = (\n",
    "  cpog_df\n",
    "  .pivot(index=\"generator_val\", columns=\"subset_label\", values=\"count_val\")\n",
    "  .fillna(0)\n",
    ")\n",
    "\n",
    "pivot_cpog_df = pivot_cpog_df.reindex(index=ordered_generators, fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "  pivot_cpog_df.T,\n",
    "  annot=True,\n",
    "  cmap=white_cmap,\n",
    "  cbar=False,\n",
    "  linecolor=\"black\",\n",
    "  linewidths=1.0,\n",
    "  annot_kws={\"size\": 20},\n",
    "  ax=ax\n",
    ")\n",
    "ax.set_title(\"Distribution of CPOG Errors by Message and Generator\", fontsize=14, pad=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.set_xlabel(\"Generator Name\", fontsize=12)\n",
    "ax.set_ylabel(\"CPOG Message\", fontsize=12)\n",
    "plt.xticks(rotation=30, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:53:05.541926Z",
     "start_time": "2025-01-11T14:52:18.888396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pivot table combining count_matches, timed_out, and cpog_message\n",
    "behavior_matrix = merged_data.pivot_table(\n",
    "   index=[\"generator\", \"instance_name\"],\n",
    "   columns=\"counter\", \n",
    "   values=[\"count_matches\", \"timed_out\", \"cpog_message\", \"crashed\"],\n",
    "   aggfunc=\"first\"\n",
    ").fillna(\"missing\")\n",
    "\n",
    "# Initialize list to store similarity data\n",
    "similarity_data = []\n",
    "\n",
    "# Calculate similarities between generators with different metrics\n",
    "for i, gen1 in enumerate(ordered_generators):\n",
    "   for j, gen2 in enumerate(ordered_generators):\n",
    "       if i >= j: # Only compute upper triangle\n",
    "           continue\n",
    "           \n",
    "       gen1_behavior = behavior_matrix[behavior_matrix.index.get_level_values(\"generator\") == gen1]\n",
    "       gen2_behavior = behavior_matrix[behavior_matrix.index.get_level_values(\"generator\") == gen2]\n",
    "       instance_similarities = []\n",
    "       \n",
    "       for idx1, row1 in gen1_behavior.iterrows():\n",
    "            for idx2, row2 in gen2_behavior.iterrows():\n",
    "                matches = (row1 == row2).sum()\n",
    "                comparisons = len(row1)\n",
    "                instance_similarity = matches / comparisons if comparisons > 0 else 0\n",
    "                instance_similarities.append(instance_similarity)\n",
    "               \n",
    "       # Calculate different aggregations\n",
    "       similarity_data.append({\n",
    "           'generator1': gen1,\n",
    "           'generator2': gen2,\n",
    "           'mean_similarity': np.mean(instance_similarities),\n",
    "           'median_similarity': np.median(instance_similarities),\n",
    "           'min_similarity': np.min(instance_similarities),\n",
    "           'max_similarity': np.max(instance_similarities),\n",
    "           'std_similarity': np.std(instance_similarities),\n",
    "           'q25_similarity': np.percentile(instance_similarities, 25),\n",
    "           'q75_similarity': np.percentile(instance_similarities, 75),\n",
    "       })\n",
    "\n",
    "# Convert to DataFrame\n",
    "similarity_features_df = pd.DataFrame(similarity_data)\n",
    "\n",
    "# Create symmetric version by adding reversed pairs\n",
    "reversed_pairs = similarity_features_df.copy()\n",
    "reversed_pairs[['generator1', 'generator2']] = reversed_pairs[['generator2', 'generator1']]\n",
    "similarity_features_df = pd.concat([similarity_features_df, reversed_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for each metric\n",
    "metrics = ['mean_similarity', 'median_similarity']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "fig.suptitle(\"Generator Similarity Analysis\", fontsize=14, y=1.05)\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    # Create similarity matrix for this metric\n",
    "    pivot_df = similarity_features_df.pivot(index='generator1',\n",
    "                                          columns='generator2',\n",
    "                                          values=metric)\n",
    "    \n",
    "    # Reindex with ordered generators\n",
    "    pivot_df = pivot_df.reindex(index=ordered_generators,\n",
    "                               columns=ordered_generators)\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(pivot_df, dtype=bool))\n",
    "    \n",
    "    # Create heatmap - only show cbar for the last plot\n",
    "    sns.heatmap(pivot_df,\n",
    "                xticklabels=ordered_generators,\n",
    "                yticklabels=ordered_generators if idx == 0 else [],\n",
    "                cmap=plt.cm.viridis,\n",
    "                mask=mask,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                square=True,\n",
    "                ax=axes[idx],\n",
    "                cbar=idx == 1)  # Changed from idx == 2 to idx == 1 since we only have 2 plots\n",
    "    \n",
    "    # Force aspect ratio to be equal\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].set_title(metric)\n",
    "    axes[idx].set_xticklabels(ordered_generators, rotation=45, ha=\"right\")\n",
    "\n",
    "# Adjust layout while maintaining aspect ratio\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:53:31.608225Z",
     "start_time": "2025-01-11T14:53:05.750859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select numeric features\n",
    "exclude_cols = [\"counter\", \"satisfiability\", \"problem_type\", \"timed_out\",\n",
    "                \"error\", \"generator\", \"verified\", \"cpog_message\", \"cpog_count\",\n",
    "                \"count_matches\", \"solved\", \"seed\", \"result_type\"]\n",
    "feature_cols = [col for col in merged_data.columns \n",
    "                if col not in exclude_cols and \n",
    "                merged_data[col].dtype in [\"float64\", \"int64\"]]\n",
    "\n",
    "# Get similarity metrics and their specific analysis methods\n",
    "similarity_metrics = {\n",
    "    'mean_similarity': 'median',  # use median for robustness\n",
    "    'median_similarity': 'median',  # natural split point\n",
    "    'min_similarity': 'quartile',  # compare bottom 25% vs top 25%\n",
    "    'max_similarity': 'quartile',  # compare bottom 25% vs top 25%\n",
    "    'std_similarity': 'quartile',  # compare high/low variance groups\n",
    "    'q25_similarity': 'fixed',  # use q25 value as fixed threshold\n",
    "    'q75_similarity': 'fixed'   # use q75 value as fixed threshold\n",
    "}\n",
    "\n",
    "all_metric_results = {}\n",
    "\n",
    "for similarity_metric, threshold_type in similarity_metrics.items():\n",
    "    feature_stats = []\n",
    "    max_sample = 1000\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        feature_diffs = []\n",
    "        similarities = []\n",
    "        \n",
    "        # Compute feature differences between generators\n",
    "        for i, gen1 in enumerate(ordered_generators):\n",
    "            for j, gen2 in enumerate(ordered_generators[i+1:], i+1):\n",
    "                sim_score = similarity_features_df[\n",
    "                    (similarity_features_df['generator1'] == gen1) & \n",
    "                    (similarity_features_df['generator2'] == gen2)\n",
    "                ][similarity_metric].iloc[0]\n",
    "                \n",
    "                gen1_data = merged_data[merged_data[\"generator\"] == gen1][feature]\n",
    "                gen2_data = merged_data[merged_data[\"generator\"] == gen2][feature]\n",
    "                \n",
    "                if len(gen1_data) > max_sample:\n",
    "                    gen1_data = gen1_data.sample(max_sample)\n",
    "                if len(gen2_data) > max_sample:\n",
    "                    gen2_data = gen2_data.sample(max_sample)\n",
    "                \n",
    "                diffs = np.abs(gen1_data.values[:, np.newaxis] - gen2_data.values[np.newaxis, :]).flatten()\n",
    "                \n",
    "                if len(diffs) > max_sample:\n",
    "                    diffs = np.random.choice(diffs, max_sample, replace=False)\n",
    "                \n",
    "                feature_diffs.extend(diffs)\n",
    "                similarities.extend([sim_score] * len(diffs))\n",
    "        \n",
    "        # Skip if no data collected\n",
    "        if not feature_diffs or not similarities:\n",
    "            continue\n",
    "            \n",
    "        feature_diffs = np.array(feature_diffs)\n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        # Apply different thresholding strategies\n",
    "        if threshold_type == 'median':\n",
    "            threshold = np.median(similarities)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "            \n",
    "        elif threshold_type == 'quartile':\n",
    "            q75, q25 = np.percentile(similarities, [75, 25])\n",
    "            high_sim_mask = similarities >= q75\n",
    "            low_sim_mask = similarities <= q25\n",
    "            \n",
    "        elif threshold_type == 'fixed':\n",
    "            if similarity_metric == 'q25_similarity':\n",
    "                threshold = np.percentile(similarities, 25)\n",
    "            else:  # q75_similarity\n",
    "                threshold = np.percentile(similarities, 75)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "        \n",
    "        high_sim = feature_diffs[high_sim_mask]\n",
    "        low_sim = feature_diffs[low_sim_mask]\n",
    "        \n",
    "        min_samples = 30\n",
    "        if len(high_sim) < min_samples or len(low_sim) < min_samples:\n",
    "            continue\n",
    "            \n",
    "        sample_size = min(len(high_sim), len(low_sim), max_sample)\n",
    "        high_sim = np.random.choice(high_sim, sample_size, replace=False)\n",
    "        low_sim = np.random.choice(low_sim, sample_size, replace=False)\n",
    "        \n",
    "        if len(np.unique(high_sim)) == 1 or len(np.unique(low_sim)) == 1:\n",
    "            continue\n",
    "            \n",
    "        statistic, p_value = stats.mannwhitneyu(high_sim, low_sim, alternative='two-sided')\n",
    "        cliff_delta = abs((2 * statistic - sample_size * sample_size) / (sample_size * sample_size))\n",
    "        \n",
    "        feature_stats.append({\n",
    "            \"feature\": feature,\n",
    "            \"p_value\": p_value,\n",
    "            \"effect_size\": cliff_delta,\n",
    "            \"test_type\": \"Mann-Whitney\",\n",
    "            \"threshold_type\": threshold_type\n",
    "        })\n",
    "    \n",
    "    # Skip if no statistics were collected\n",
    "    if not feature_stats:\n",
    "        print(f\"\\nNo significant features found for {similarity_metric}\")\n",
    "        continue\n",
    "        \n",
    "    alpha = 0.05\n",
    "    feature_stats_df = pd.DataFrame(feature_stats)\n",
    "    feature_stats_df[\"significant\"] = feature_stats_df[\"p_value\"] < (alpha / len(feature_cols))\n",
    "    feature_stats_df = feature_stats_df.sort_values(\"effect_size\", ascending=False)\n",
    "    \n",
    "    significant_features = feature_stats_df[feature_stats_df[\"significant\"]]\n",
    "    if not significant_features.empty:\n",
    "        all_metric_results[similarity_metric] = significant_features\n",
    "    else:\n",
    "        print(f\"\\nNo significant features found for {similarity_metric} after correction\")\n",
    "\n",
    "# Print top significant features for each similarity metric\n",
    "for metric, results_df in all_metric_results.items():\n",
    "    print(f\"\\nTop significant features for {metric} (using {similarity_metrics[metric]} thresholding):\")\n",
    "    print(results_df.head(5)[[\"feature\", \"effect_size\", \"test_type\", \"threshold_type\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for each similarity metric\n",
    "for metric, results_df in all_metric_results.items():\n",
    "    top_features = results_df[results_df[\"significant\"]].head(5)[\"feature\"].tolist()\n",
    "    if not top_features:\n",
    "        continue\n",
    "        \n",
    "    n_features = len(top_features)\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(15, 5*n_features))\n",
    "    \n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Create proper viridis colors\n",
    "    viridis = plt.cm.viridis\n",
    "    colors = [viridis(0.2), viridis(0.8)]\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        # Recompute feature differences and similarities\n",
    "        feature_diffs = []\n",
    "        similarities = []\n",
    "        \n",
    "        for i, gen1 in enumerate(ordered_generators):\n",
    "            for j, gen2 in enumerate(ordered_generators[i+1:], i+1):\n",
    "                # Get similarity score for this generator pair\n",
    "                sim_score = similarity_features_df[\n",
    "                    (similarity_features_df[\"generator1\"] == gen1) & \n",
    "                    (similarity_features_df[\"generator2\"] == gen2)\n",
    "                ][metric].iloc[0]\n",
    "                \n",
    "                # Get feature data for both generators\n",
    "                gen1_data = merged_data[merged_data[\"generator\"] == gen1][feature]\n",
    "                gen2_data = merged_data[merged_data[\"generator\"] == gen2][feature]\n",
    "                \n",
    "                # Compute pairwise absolute differences\n",
    "                diffs = np.abs(gen1_data.values[:, np.newaxis] - gen2_data.values[np.newaxis, :]).flatten()\n",
    "                \n",
    "                # Store differences and corresponding similarity\n",
    "                feature_diffs.extend(diffs)\n",
    "                similarities.extend([sim_score] * len(diffs))\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        feature_diffs = np.array(feature_diffs)\n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        # Apply the same thresholding strategy as in the analysis\n",
    "        threshold_type = similarity_metrics[metric]\n",
    "        if threshold_type == \"median\":\n",
    "            threshold = np.median(similarities)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "        elif threshold_type == \"quartile\":\n",
    "            q75, q25 = np.percentile(similarities, [75, 25])\n",
    "            high_sim_mask = similarities >= q75\n",
    "            low_sim_mask = similarities <= q25\n",
    "        elif threshold_type == \"fixed\":\n",
    "            if metric == \"q25_similarity\":\n",
    "                threshold = np.percentile(similarities, 25)\n",
    "            else:  # q75_similarity\n",
    "                threshold = np.percentile(similarities, 75)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "            \n",
    "        high_sim_data = feature_diffs[high_sim_mask]\n",
    "        low_sim_data = feature_diffs[low_sim_mask]\n",
    "        \n",
    "        # Boxplot\n",
    "        ax_box = axes[idx, 0]\n",
    "        bp = ax_box.boxplot([low_sim_data, high_sim_data],\n",
    "                          tick_labels=[\"Low Similarity\", \"High Similarity\"],\n",
    "                          patch_artist=True)\n",
    "        \n",
    "        # Set proper viridis colors\n",
    "        for patch, color in zip(bp[\"boxes\"], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "            \n",
    "        plt.setp(bp[\"whiskers\"], color=colors[0])\n",
    "        plt.setp(bp[\"caps\"], color=colors[0])\n",
    "        plt.setp(bp[\"medians\"], color=\"white\")\n",
    "        plt.setp(bp[\"fliers\"], markerfacecolor=colors[0])\n",
    "        \n",
    "        ax_box.set_title(f\"{feature} - Boxplot\")\n",
    "        \n",
    "        # Density plot\n",
    "        ax_density = axes[idx, 1]\n",
    "        x_range = np.linspace(min(feature_diffs), max(feature_diffs), 200)\n",
    "        \n",
    "        if len(np.unique(low_sim_data)) > 1:\n",
    "            kde_low = stats.gaussian_kde(low_sim_data)\n",
    "            ax_density.fill_between(x_range, kde_low(x_range),\n",
    "                                  alpha=0.6, color=colors[0],\n",
    "                                  label=\"Low Similarity\")\n",
    "            \n",
    "        if len(np.unique(high_sim_data)) > 1:\n",
    "            kde_high = stats.gaussian_kde(high_sim_data)\n",
    "            ax_density.fill_between(x_range, kde_high(x_range),\n",
    "                                  alpha=0.6, color=colors[1],\n",
    "                                  label=\"High Similarity\")\n",
    "            \n",
    "        ax_density.set_title(f\"{feature} - Density Plot\")\n",
    "        ax_density.legend()\n",
    "    \n",
    "    fig.suptitle(f\"Feature Distributions for {metric}\\n(using {threshold_type} thresholding)\", \n",
    "                 fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical results\n",
    "    print(f\"\\nTop features by effect size for {metric} (statistically significant):\")\n",
    "    print(results_df[[\"feature\", \"test_type\", \"effect_size\", \"p_value\", \"threshold_type\"]].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global font sizes for all plots\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "# Create visualizations for each similarity metric\n",
    "for metric, results_df in all_metric_results.items():\n",
    "    # Get only the first 2 significant features\n",
    "    top_features = results_df[results_df[\"significant\"]].head(2)[\"feature\"].tolist()\n",
    "    if not top_features:\n",
    "        continue\n",
    "        \n",
    "    n_features = len(top_features)\n",
    "    # Increased figure size and adjusted height per feature\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(16, 6*n_features))\n",
    "    \n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Create proper viridis colors with increased contrast\n",
    "    viridis = plt.cm.viridis\n",
    "    colors = [viridis(0.15), viridis(0.85)]\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        # [Previous feature difference and similarity computation code remains the same]\n",
    "        feature_diffs = []\n",
    "        similarities = []\n",
    "        \n",
    "        for i, gen1 in enumerate(ordered_generators):\n",
    "            for j, gen2 in enumerate(ordered_generators[i+1:], i+1):\n",
    "                sim_score = similarity_features_df[\n",
    "                    (similarity_features_df[\"generator1\"] == gen1) & \n",
    "                    (similarity_features_df[\"generator2\"] == gen2)\n",
    "                ][metric].iloc[0]\n",
    "                \n",
    "                gen1_data = merged_data[merged_data[\"generator\"] == gen1][feature]\n",
    "                gen2_data = merged_data[merged_data[\"generator\"] == gen2][feature]\n",
    "                \n",
    "                diffs = np.abs(gen1_data.values[:, np.newaxis] - gen2_data.values[np.newaxis, :]).flatten()\n",
    "                \n",
    "                feature_diffs.extend(diffs)\n",
    "                similarities.extend([sim_score] * len(diffs))\n",
    "        \n",
    "        feature_diffs = np.array(feature_diffs)\n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        # Apply thresholding\n",
    "        threshold_type = similarity_metrics[metric]\n",
    "        if threshold_type == \"median\":\n",
    "            threshold = np.median(similarities)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "        elif threshold_type == \"quartile\":\n",
    "            q75, q25 = np.percentile(similarities, [75, 25])\n",
    "            high_sim_mask = similarities >= q75\n",
    "            low_sim_mask = similarities <= q25\n",
    "        elif threshold_type == \"fixed\":\n",
    "            if metric == \"q25_similarity\":\n",
    "                threshold = np.percentile(similarities, 25)\n",
    "            else:  # q75_similarity\n",
    "                threshold = np.percentile(similarities, 75)\n",
    "            high_sim_mask = similarities > threshold\n",
    "            low_sim_mask = similarities <= threshold\n",
    "            \n",
    "        high_sim_data = feature_diffs[high_sim_mask]\n",
    "        low_sim_data = feature_diffs[low_sim_mask]\n",
    "        \n",
    "        # Enhanced Boxplot\n",
    "        ax_box = axes[idx, 0]\n",
    "        bp = ax_box.boxplot([low_sim_data, high_sim_data],\n",
    "                          tick_labels=[\"Low Similarity\", \"High Similarity\"],\n",
    "                          patch_artist=True,\n",
    "                          medianprops=dict(color=\"white\", linewidth=1.5),\n",
    "                          flierprops=dict(marker='o', markerfacecolor=colors[0], markersize=8),\n",
    "                          widths=0.7)\n",
    "        \n",
    "        # Set proper viridis colors with higher opacity\n",
    "        for patch, color in zip(bp[\"boxes\"], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "            \n",
    "        plt.setp(bp[\"whiskers\"], color=\"black\", linewidth=1.5)\n",
    "        plt.setp(bp[\"caps\"], color=\"black\", linewidth=1.5)\n",
    "        \n",
    "        ax_box.set_title(f\"{feature} - Boxplot\", pad=15)\n",
    "        ax_box.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Enhanced Density plot\n",
    "        ax_density = axes[idx, 1]\n",
    "        x_range = np.linspace(min(feature_diffs), max(feature_diffs), 200)\n",
    "        \n",
    "        if len(np.unique(low_sim_data)) > 1:\n",
    "            kde_low = stats.gaussian_kde(low_sim_data)\n",
    "            ax_density.fill_between(x_range, kde_low(x_range),\n",
    "                                  alpha=0.7, color=colors[0],\n",
    "                                  label=\"Low Similarity\")\n",
    "            \n",
    "        if len(np.unique(high_sim_data)) > 1:\n",
    "            kde_high = stats.gaussian_kde(high_sim_data)\n",
    "            ax_density.fill_between(x_range, kde_high(x_range),\n",
    "                                  alpha=0.7, color=colors[1],\n",
    "                                  label=\"High Similarity\")\n",
    "            \n",
    "        ax_density.set_title(f\"{feature} - Density Plot\", pad=15)\n",
    "        ax_density.legend(frameon=True, edgecolor='black')\n",
    "        ax_density.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Enhanced title and layout\n",
    "    fig.suptitle(f\"Feature Distributions for {metric}\\n(using {threshold_type} thresholding)\", \n",
    "                 fontsize=16, y=1.04)\n",
    "    \n",
    "    # Add more spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88, hspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical results for only top 2 features\n",
    "    print(f\"\\nTop 2 features by effect size for {metric} (statistically significant):\")\n",
    "    print(results_df[[\"feature\", \"test_type\", \"effect_size\", \"p_value\", \"threshold_type\"]].head(2).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
