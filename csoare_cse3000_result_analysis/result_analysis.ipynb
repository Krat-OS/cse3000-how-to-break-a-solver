{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:16.434201Z",
     "start_time": "2025-01-11T14:52:15.700989Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, gaussian_kde\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from result_analysis.helper_functions import process_csv_data, get_numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:52:16.641344Z",
     "start_time": "2025-01-11T14:52:16.498314Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_results = process_csv_data(\"path/to/fuzzing_output.csv\"),\n",
    "experiment_features = process_csv_data(\"path/to/features.csv\"),\n",
    "\n",
    "def merge_data(\n",
    "    fuzzing_results: pd.DataFrame,\n",
    "    satzilla_features: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge invalid fuzzer instances with Satzilla features, based on their index\n",
    "    (which should be set to 'instance_name' in both data frames).\n",
    "    \n",
    "    Args:\n",
    "        fuzzing_results (pd.DataFrame): DataFrame of invalid fuzzer instances.\n",
    "        satzilla_features (pd.DataFrame): DataFrame of Satzilla features.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame, containing rows that appear in both.\n",
    "    \"\"\"\n",
    "    if \"instance_name\" in fuzzing_results.columns:\n",
    "        fuzzing_results = fuzzing_results.set_index(\"instance_name\")\n",
    "    if \"instance_name\" in satzilla_features.columns:\n",
    "        satzilla_features = satzilla_features.set_index(\"instance_name\")\n",
    "    \n",
    "    cols_to_drop = ['base_generator', 'presumed_difficulty', 'randomness', 'generator']\n",
    "    fuzzing_results = fuzzing_results.drop(columns=cols_to_drop)\n",
    "    \n",
    "    merged_df = fuzzing_results.join(satzilla_features, how=\"inner\")\n",
    "    return merged_df\n",
    "\n",
    "merged_data = merge_data(experiment_results, experiment_features)\n",
    "\n",
    "print(merged_data[\"generator\"].value_counts())\n",
    "print(merged_data[\"counter\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_NaN_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix NaN values in satisfiability column based on count_value:\n",
    "    - If count_value > 0, set satisfiability to 'SATISFIABLE'\n",
    "    - If count_value = 0, set satisfiability to 'UNSATISFIABLE'\n",
    "    Only affects rows where satisfiability is NaN but count_value is not NaN.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing satisfiability and count_value columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with fixed satisfiability values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['count_value'] = pd.to_numeric(df['count_value'], errors='coerce')\n",
    "    \n",
    "    mask = df['satisfiability'].isna() & df['count_value'].notna()\n",
    "    \n",
    "    df.loc[mask & (df['count_value'] > 0), 'satisfiability'] = 'SATISFIABLE'\n",
    "    df.loc[mask & (df['count_value'] == 0), 'satisfiability'] = 'UNSATISFIABLE'\n",
    "    \n",
    "    return df\n",
    "\n",
    "fix_NaN_columns(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_cpog(merged_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Classify fuzzing results using CPOG verification.\n",
    "    \"\"\"\n",
    "    correct_mask = merged_df[\"count_matches\"] == True\n",
    "    correct_results = merged_df[correct_mask].copy()\n",
    "    remaining_df = merged_df[~correct_mask].copy()\n",
    "    \n",
    "    incorrect_mask = (remaining_df[\"count_matches\"] == False) & (remaining_df[\"cpog_message\"] == \"NO ERROR\")\n",
    "    incorrect_results = remaining_df[incorrect_mask].copy()\n",
    "    remaining_df = remaining_df[~incorrect_mask].copy()\n",
    "    \n",
    "    cpog_errors_list = []\n",
    "    if not remaining_df.empty:\n",
    "        instance_groups = remaining_df.groupby(level=0)\n",
    "        additional_incorrect = pd.DataFrame()\n",
    "        consistent_cpog_errors = pd.DataFrame()\n",
    "        \n",
    "        for _, group in instance_groups:\n",
    "            unique_counts = group[\"count_value\"].unique()\n",
    "            if len(unique_counts) > 1:\n",
    "                count_value_counts = group[\"count_value\"].value_counts()\n",
    "                majority_count = count_value_counts.index[0]\n",
    "                incorrect_mask = group[\"count_value\"] != majority_count\n",
    "                additional_incorrect = pd.concat([additional_incorrect, group[incorrect_mask]])\n",
    "            else:\n",
    "                consistent_cpog_errors = pd.concat([consistent_cpog_errors, group])\n",
    "        \n",
    "        incorrect_results = pd.concat([incorrect_results, additional_incorrect])\n",
    "        \n",
    "        if not consistent_cpog_errors.empty:\n",
    "            for msg in consistent_cpog_errors[\"cpog_message\"].unique():\n",
    "                if pd.notna(msg):\n",
    "                    subset = consistent_cpog_errors[consistent_cpog_errors[\"cpog_message\"] == msg].copy()\n",
    "                    cpog_errors_list.append(subset)\n",
    "    \n",
    "    return [correct_results, incorrect_results] + cpog_errors_list\n",
    "\n",
    "def classify_with_majority_vote(merged_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Classify fuzzing results using majority vote across counters.\n",
    "    \"\"\"\n",
    "    instance_groups = merged_df.groupby(level=0)\n",
    "    majority_votes = {}\n",
    "    \n",
    "    for instance_name, group in instance_groups:\n",
    "        count_value_counts = group[\"count_value\"].value_counts()\n",
    "        majority_count = count_value_counts.index[0]\n",
    "        majority_votes[instance_name] = majority_count\n",
    "    \n",
    "    merged_df[\"correct_count_majority_vote\"] = merged_df.index.map(majority_votes)\n",
    "    merged_df[\"count_matches_majority\"] = merged_df[\"count_value\"] == merged_df[\"correct_count_majority_vote\"]\n",
    "    \n",
    "    correct_mask = merged_df[\"count_matches_majority\"] == True\n",
    "    correct_results = merged_df[correct_mask].copy()\n",
    "    incorrect_results = merged_df[~correct_mask].copy()\n",
    "    \n",
    "    return [correct_results, incorrect_results]\n",
    "def classify_fuzzing_results(merged_df: pd.DataFrame) -> Tuple[List[pd.DataFrame], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Classify fuzzing results into categories and add classification labels.\n",
    "    \n",
    "    Args:\n",
    "        merged_df (pd.DataFrame): DataFrame containing fuzzing results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[pd.DataFrame], pd.DataFrame]: A tuple containing:\n",
    "        1. List of DataFrames for each category:\n",
    "           - timeout_results\n",
    "           - crash_results\n",
    "           - correct_results\n",
    "           - incorrect_results\n",
    "           followed by DataFrames for each CPOG error message\n",
    "        2. Original DataFrame with added 'result_classification' column\n",
    "    \"\"\"\n",
    "    df = merged_df.copy()\n",
    "    df['result_classification'] = 'unknown'\n",
    "    \n",
    "    timeout_mask = df[\"timed_out\"] == True if \"timed_out\" in df.columns else pd.Series(False, index=df.index)\n",
    "    timeout_results = df[timeout_mask].copy()\n",
    "    df.loc[timeout_mask, 'result_classification'] = 'timeout'\n",
    "    \n",
    "    crash_mask = df[\"count_value\"].isna() & ~timeout_mask\n",
    "    crash_results = df[crash_mask].copy()\n",
    "    df.loc[crash_mask, 'result_classification'] = 'crash'\n",
    "    \n",
    "    remaining_mask = ~timeout_mask & ~crash_mask\n",
    "    remaining_df = df[remaining_mask].copy()\n",
    "    \n",
    "    has_cpog = all(col in remaining_df.columns for col in [\"cpog_message\", \"cpog_count\", \"count_matches\"])\n",
    "    \n",
    "    if has_cpog:\n",
    "        classification_results = classify_with_cpog(remaining_df)\n",
    "        correct_results, incorrect_results = classification_results[:2]\n",
    "        cpog_error_results = classification_results[2:]\n",
    "        \n",
    "        for result_df, label in [(correct_results, 'correct'), \n",
    "                               (incorrect_results, 'incorrect')]:\n",
    "            result_mask = df.index.isin(result_df.index)\n",
    "            df.loc[result_mask & remaining_mask, 'result_classification'] = label\n",
    "        \n",
    "        for error_df in cpog_error_results:\n",
    "            if not error_df.empty:\n",
    "                error_msg = error_df['cpog_message'].iloc[0]\n",
    "                error_mask = df.index.isin(error_df.index)\n",
    "                df.loc[error_mask & remaining_mask, 'result_classification'] = f'cpog_error_{error_msg}'\n",
    "        \n",
    "        results = [timeout_results, crash_results, correct_results, incorrect_results] + cpog_error_results\n",
    "    else:\n",
    "        classification_results = classify_with_majority_vote(remaining_df)\n",
    "        correct_results, incorrect_results = classification_results\n",
    "        \n",
    "        for result_df, label in [(correct_results, 'correct'), \n",
    "                               (incorrect_results, 'incorrect')]:\n",
    "            result_mask = df.index.isin(result_df.index)\n",
    "            df.loc[result_mask & remaining_mask, 'result_classification'] = label\n",
    "        \n",
    "        results = [timeout_results, crash_results, correct_results, incorrect_results]\n",
    "    \n",
    "    return results, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results, classified_df) = classify_fuzzing_results(merged_data)\n",
    "timeout_results, crash_results, correct_results, incorrect_results = results[:4]\n",
    "cpog_subsets = results[4:] if len(results) > 4 else []\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Timeouts: {len(timeout_results)}\")\n",
    "print(f\"Crashes: {len(crash_results)}\")\n",
    "print(f\"Correct: {len(correct_results)}\")\n",
    "print(f\"Incorrect: {len(incorrect_results)}\")\n",
    "\n",
    "print(\"\\nBreakdown by Generator:\")\n",
    "print(\"-\" * 20)\n",
    "for generator in sorted(merged_data['generator'].unique()):\n",
    "    print(f\"\\nGenerator: {generator}\")\n",
    "    gen_mask = merged_data['generator'] == generator\n",
    "    print(f\"Timeouts: {len(timeout_results[timeout_results['generator'] == generator])}\")\n",
    "    print(f\"Crashes: {len(crash_results[crash_results['generator'] == generator])}\")\n",
    "    print(f\"Correct: {len(correct_results[correct_results['generator'] == generator])}\")\n",
    "    print(f\"Incorrect: {len(incorrect_results[incorrect_results['generator'] == generator])}\")\n",
    "\n",
    "print(\"\\nBreakdown by Solver:\")\n",
    "print(\"-\" * 20)\n",
    "for solver in sorted(merged_data['counter'].unique()):\n",
    "    print(f\"\\nSolver: {solver}\")\n",
    "    solver_mask = merged_data['counter'] == solver\n",
    "    print(f\"Timeouts: {len(timeout_results[timeout_results['counter'] == solver])}\")\n",
    "    print(f\"Crashes: {len(crash_results[crash_results['counter'] == solver])}\")\n",
    "    print(f\"Correct: {len(correct_results[correct_results['counter'] == solver])}\")\n",
    "    print(f\"Incorrect: {len(incorrect_results[incorrect_results['counter'] == solver])}\")\n",
    "\n",
    "if cpog_subsets:\n",
    "    print(\"\\nCPOG error categories:\")\n",
    "    print(\"-\" * 20)\n",
    "    for subset in cpog_subsets:\n",
    "        msg = subset['cpog_message'].iloc[0]\n",
    "        print(f\"\\nCPOG error '{msg}':\")\n",
    "        \n",
    "        print(\"\\nBy Generator:\")\n",
    "        for generator in sorted(subset['generator'].unique()):\n",
    "            count = len(subset[subset['generator'] == generator])\n",
    "            print(f\"{generator}: {count} instances\")\n",
    "            \n",
    "        print(\"\\nBy Solver:\")\n",
    "        for solver in sorted(subset['counter'].unique()):\n",
    "            count = len(subset[subset['counter'] == solver])\n",
    "            print(f\"{solver}: {count} instances\")\n",
    "else:\n",
    "    print(\"\\nUsing majority vote classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_solve_time = classified_df.groupby('generator')['solve_time'].mean().sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=avg_solve_time.index, y=avg_solve_time.values, palette='viridis', width=0.4)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "plt.xlabel('Generator', fontsize=16)\n",
    "plt.ylabel('Average Solve Time', fontsize=18) \n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_count_value = classified_df.groupby('generator')['count_value'].mean().sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=avg_count_value.index, y=avg_count_value.values, palette='viridis', width=0.4)\n",
    "plt.yscale('log')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "plt.xlabel('Generator', fontsize=16)\n",
    "plt.ylabel('Average Count Value (log scale)', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_categories = {\n",
    "   'Problem Size': ['nvars', 'nclauses', 'vars-clauses-ratio', 'nvarsOrig', 'nclausesOrig'],\n",
    "   'Variable-Clause Graph': ['VCG-VAR-mean', 'VCG-VAR-coeff-variation', 'VCG-VAR-min', 'VCG-VAR-max', 'VCG-VAR-entropy', \n",
    "                             'VCG-CLAUSE-mean', 'VCG-CLAUSE-coeff-variation', 'VCG-CLAUSE-min', 'VCG-CLAUSE-max', 'VCG-CLAUSE-entropy'],\n",
    "   'Variable Graph': ['VG-mean', 'VG-coeff-variation', 'VG-min', 'VG-max'],\n",
    "   'Clause Graph': ['CG-mean', 'CG-coeff-variation', 'CG-min', 'CG-max', 'CG-entropy',\n",
    "                    'cluster-coeff-mean', 'cluster-coeff-coeff-variation', 'cluster-coeff-min', 'cluster-coeff-max', 'cluster-coeff-entropy'],\n",
    "   'Balance': ['POSNEG-RATIO-CLAUSE-mean', 'POSNEG-RATIO-CLAUSE-coeff-variation', 'POSNEG-RATIO-CLAUSE-min', 'POSNEG-RATIO-CLAUSE-max', 'POSNEG-RATIO-CLAUSE-entropy',\n",
    "              'POSNEG-RATIO-VAR-mean', 'POSNEG-RATIO-VAR-stdev', 'POSNEG-RATIO-VAR-min', 'POSNEG-RATIO-VAR-max', 'POSNEG-RATIO-VAR-entropy',\n",
    "              'UNARY', 'BINARY+', 'TRINARY+'],\n",
    "   'Horn Formula': ['horn-clauses-fraction', 'HORNY-VAR-mean', 'HORNY-VAR-coeff-variation', 'HORNY-VAR-min', 'HORNY-VAR-max', 'HORNY-VAR-entropy'],\n",
    "   'Other Features': ['Pre-featuretime', 'Basic-featuretime', 'KLB-featuretime', 'CG-featuretime', 'solved', 'generator', 'seed', 'base_generator', 'presumed_difficulty', 'randomness']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_horn_impact(df, base_gen, horn_gen, feature_categories):\n",
    "   df1 = df[df['generator'] == base_gen].copy()\n",
    "   df2 = df[df['generator'] == horn_gen].copy()\n",
    "   feature_cols = get_numeric_features(df)\n",
    "   \n",
    "   results = []\n",
    "   for feature in feature_cols:\n",
    "       if df1[feature].isna().any() or df2[feature].isna().any():\n",
    "           continue\n",
    "           \n",
    "       if df1[feature].std() == 0 or df2[feature].std() == 0:\n",
    "           continue\n",
    "           \n",
    "       try:\n",
    "           mean_diff = df2[feature].mean() - df1[feature].mean()\n",
    "           if df1[feature].mean() != 0:\n",
    "               mean_diff_pct = mean_diff / df1[feature].mean() * 100\n",
    "           else:\n",
    "               mean_diff_pct = float('inf')\n",
    "           \n",
    "           combined_data = pd.concat([df1, df2])\n",
    "           corr = spearmanr(combined_data[feature], combined_data['solve_time'])\n",
    "           \n",
    "           category = 'Other'\n",
    "           for cat, features in feature_categories.items():\n",
    "               if feature in features:\n",
    "                   category = cat\n",
    "                   break\n",
    "                   \n",
    "           results.append({\n",
    "               'feature': feature,\n",
    "               'category': category,\n",
    "               'mean_difference': mean_diff,\n",
    "               'percent_change': mean_diff_pct,\n",
    "               'solve_time_correlation': corr.correlation,\n",
    "               'correlation_p_value': corr.pvalue\n",
    "           })\n",
    "       except:\n",
    "           continue\n",
    "\n",
    "   return pd.DataFrame(results)\n",
    "\n",
    "fuzz_results = compare_horn_impact(\n",
    "   classified_df,\n",
    "   'FuzzSAT-hard-100',\n",
    "   'FuzzSATHORN-hard-100',\n",
    "   feature_categories\n",
    ")\n",
    "\n",
    "pair_results = compare_horn_impact(\n",
    "   classified_df,\n",
    "   'PairSAT-hard-100',\n",
    "   'PairSATHORN-hard-100',\n",
    "   feature_categories\n",
    ")\n",
    "\n",
    "sig_fuzz = fuzz_results[(fuzz_results['correlation_p_value'] < 0.05) & \n",
    "                       (abs(fuzz_results['solve_time_correlation']) > 0.4)]\n",
    "sig_pair = pair_results[(pair_results['correlation_p_value'] < 0.05) & \n",
    "                       (abs(pair_results['solve_time_correlation']) > 0.4)]\n",
    "\n",
    "common_features = set(sig_fuzz['feature']) & set(sig_pair['feature'])\n",
    "\n",
    "print(\"Features with significant correlation in both FuzzSAT and PairSAT:\\n\")\n",
    "for category in feature_categories.keys():\n",
    "   cat_features = [f for f in common_features if f in feature_categories[category]]\n",
    "   if cat_features:\n",
    "       print(f\"\\n{category}:\")\n",
    "       print(\"=\" * 80)\n",
    "       for feature in cat_features:\n",
    "           fuzz_corr = sig_fuzz[sig_fuzz['feature'] == feature]['solve_time_correlation'].iloc[0]\n",
    "           pair_corr = sig_pair[sig_pair['feature'] == feature]['solve_time_correlation'].iloc[0]\n",
    "           print(f\"{feature:40} FuzzSAT: {fuzz_corr:6.3f}  PairSAT: {pair_corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:53:05.541926Z",
     "start_time": "2025-01-11T14:52:18.888396Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_behavior_probabilities(solver_data: pd.DataFrame) -> dict:\n",
    "   \"\"\"\n",
    "   Calculate probability distribution of behaviors for a generator.\n",
    "   \n",
    "   Args:\n",
    "       solver_data: DataFrame with result_classification column\n",
    "       \n",
    "   Returns:\n",
    "       dict: Probabilities for each behavior type\n",
    "   \"\"\"\n",
    "   total = len(solver_data)\n",
    "   behavior_counts = solver_data['result_classification'].value_counts()\n",
    "   \n",
    "   return {\n",
    "       'correct': behavior_counts.get('correct', 0) / total,\n",
    "       'incorrect': behavior_counts.get('incorrect', 0) / total,\n",
    "       'timeout': behavior_counts.get('timeout', 0) / total,\n",
    "       'crash': behavior_counts.get('crash', 0) / total\n",
    "   }\n",
    "\n",
    "def compute_similarity(behavior1, behavior2, count1, count2, time1, time2,\n",
    "                     gen1_solver_data, gen2_solver_data):\n",
    "   \"\"\"\n",
    "   Compute similarity between two instances based on behavior, count and time.\n",
    "   \n",
    "   Args:\n",
    "       behavior1, behavior2: Classification results\n",
    "       count1, count2: Count values \n",
    "       time1, time2: Solve times\n",
    "       gen1_solver_data, gen2_solver_data: DataFrames with solver data\n",
    "       \n",
    "   Returns:\n",
    "       float: Similarity score between 0 and 1\n",
    "   \"\"\"\n",
    "   if behavior1 != behavior2:\n",
    "       return 0.0\n",
    "       \n",
    "   if behavior1 == \"timeout\" and behavior2 == \"timeout\":\n",
    "       gen1_probs = get_behavior_probabilities(gen1_solver_data)\n",
    "       gen2_probs = get_behavior_probabilities(gen2_solver_data)\n",
    "       \n",
    "       return (gen1_probs['correct'] * gen2_probs['correct'] +\n",
    "               gen1_probs['incorrect'] * gen2_probs['incorrect'])\n",
    "       \n",
    "   if behavior1 == \"crash\" and behavior2 == \"crash\":\n",
    "       if time1 is None or time2 is None:\n",
    "           return 0.0\n",
    "       return exp(-0.1 * abs(time1 - time2))\n",
    "       \n",
    "   if count1 is None or count2 is None or time1 is None or time2 is None:\n",
    "       return 0.0\n",
    "       \n",
    "   return (0.3 * exp(-0.1 * abs(count1 - count2)) + \n",
    "           0.7 * exp(-0.1 * abs(time1 - time2)))\n",
    "\n",
    "def calculate_behavioral_similarity(classified_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "   \"\"\"\n",
    "   Calculate behavioral similarity between generators.\n",
    "   \n",
    "   Args:\n",
    "       classified_df: DataFrame with generator, counter, result_classification,\n",
    "                     count_value and solve_time columns\n",
    "   \n",
    "   Returns:\n",
    "       similarity_matrix: DataFrame with pairwise generator similarities\n",
    "       stats_df: DataFrame with detailed similarity statistics\n",
    "   \"\"\"\n",
    "   generators = sorted(classified_df['generator'].unique())\n",
    "   solvers = sorted(classified_df['counter'].unique())\n",
    "   \n",
    "   similarity_matrix = pd.DataFrame(np.zeros((len(generators), len(generators))),\n",
    "                                  index=generators, columns=generators)\n",
    "   \n",
    "   similarity_stats = []\n",
    "   \n",
    "   for i, gen1 in enumerate(generators):\n",
    "       for j, gen2 in enumerate(generators[i:], i):\n",
    "           if gen1 == gen2:\n",
    "               similarity_matrix.loc[gen1, gen2] = 1.0\n",
    "               continue\n",
    "               \n",
    "           instance_similarities = []\n",
    "           \n",
    "           for solver in solvers:\n",
    "               gen1_solver_data = classified_df[\n",
    "                   (classified_df['generator'] == gen1) & \n",
    "                   (classified_df['counter'] == solver)\n",
    "               ]\n",
    "               \n",
    "               gen2_solver_data = classified_df[\n",
    "                   (classified_df['generator'] == gen2) & \n",
    "                   (classified_df['counter'] == solver)\n",
    "               ]\n",
    "               \n",
    "               gen1_instances = gen1_solver_data.index.unique()\n",
    "               gen2_instances = gen2_solver_data.index.unique()\n",
    "               \n",
    "               gen1_behaviors = dict(zip(gen1_solver_data.index, \n",
    "                                       gen1_solver_data['result_classification']))\n",
    "               gen2_behaviors = dict(zip(gen2_solver_data.index, \n",
    "                                       gen2_solver_data['result_classification']))\n",
    "               gen1_counts = dict(zip(gen1_solver_data.index,\n",
    "                                    gen1_solver_data['count_value']))\n",
    "               gen2_counts = dict(zip(gen2_solver_data.index,\n",
    "                                    gen2_solver_data['count_value']))\n",
    "               gen1_times = dict(zip(gen1_solver_data.index,\n",
    "                                   gen1_solver_data['solve_time']))\n",
    "               gen2_times = dict(zip(gen2_solver_data.index,\n",
    "                                   gen2_solver_data['solve_time']))\n",
    "               \n",
    "               for inst1 in gen1_instances:\n",
    "                   for inst2 in gen2_instances:\n",
    "                       similarity = compute_similarity(\n",
    "                           gen1_behaviors[inst1], gen2_behaviors[inst2],\n",
    "                           gen1_counts[inst1], gen2_counts[inst2],\n",
    "                           gen1_times[inst1], gen2_times[inst2],\n",
    "                           gen1_solver_data, gen2_solver_data\n",
    "                       )\n",
    "                       instance_similarities.append(similarity)\n",
    "           \n",
    "           if instance_similarities:\n",
    "               stats = {\n",
    "                   'generator1': gen1,\n",
    "                   'generator2': gen2,\n",
    "                   'mean_similarity': np.mean(instance_similarities),\n",
    "                   'median_similarity': np.median(instance_similarities),\n",
    "                   'min_similarity': np.min(instance_similarities),\n",
    "                   'max_similarity': np.max(instance_similarities),\n",
    "                   'std_similarity': np.std(instance_similarities),\n",
    "                   'q25_similarity': np.percentile(instance_similarities, 25),\n",
    "                   'q75_similarity': np.percentile(instance_similarities, 75),\n",
    "                   'count': len(instance_similarities)\n",
    "               }\n",
    "               \n",
    "               consistency = 1 - (stats['max_similarity'] - stats['min_similarity'])\n",
    "               final_score = 0.7 * stats['mean_similarity'] + 0.3 * consistency\n",
    "               \n",
    "               similarity_matrix.loc[gen1, gen2] = final_score\n",
    "               similarity_matrix.loc[gen2, gen1] = final_score\n",
    "               \n",
    "               similarity_stats.append(stats)\n",
    "               reversed_stats = stats.copy()\n",
    "               reversed_stats['generator1'], reversed_stats['generator2'] = gen2, gen1\n",
    "               similarity_stats.append(reversed_stats)\n",
    "   \n",
    "   return similarity_matrix, pd.DataFrame(similarity_stats)\n",
    "\n",
    "similarity_matrix, similarity_stats = calculate_behavioral_similarity(classified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_color(val, vmin=0, vmax=1):\n",
    "    \"\"\"\n",
    "    Determine text color based on background brightness.\n",
    "    Returns white for dark backgrounds and black for light backgrounds.\n",
    "    \"\"\"\n",
    "    threshold = (vmin + vmax) / 2\n",
    "    return 'white' if val < threshold else 'black'\n",
    "\n",
    "def sort_key(x):\n",
    "    base = x.split('-')[0]\n",
    "    diff = x.split('-')[1]\n",
    "    rand = int(x.split('-')[2])\n",
    "    diff_order = {'easy': 0, 'hard': 1}\n",
    "    return (base, diff_order[diff], rand)\n",
    "\n",
    "sorted_generators = sorted(similarity_matrix.index, key=sort_key)\n",
    "similarity_matrix = similarity_matrix.loc[sorted_generators, sorted_generators]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.rcParams.update({\n",
    "    'font.size': 24,\n",
    "    'axes.titlesize': 30,\n",
    "    'axes.labelsize': 28\n",
    "})\n",
    "\n",
    "ax = plt.axes([0.1, 0.1, 0.8, 0.8])\n",
    "mask = np.triu(np.ones_like(similarity_matrix), k=1)\n",
    "\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=False,\n",
    "    cmap='viridis',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    mask=mask,\n",
    "    square=True,\n",
    "    cbar=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(len(similarity_matrix)):\n",
    "        if not mask[i, j]:\n",
    "            val = similarity_matrix.iloc[i, j]\n",
    "            text_color = get_text_color(val)\n",
    "            ax.text(j + 0.5, i + 0.5, f'{val:.3f}',\n",
    "                   ha='center', va='center',\n",
    "                   size=22, color=text_color)\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', va='top', fontsize=20)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)\n",
    "\n",
    "plt.gcf().set_size_inches(20, 20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_instance_similarity_features(classified_df: pd.DataFrame, similarity_matrix: pd.DataFrame, \n",
    "                                       similarity_stats: pd.DataFrame):\n",
    "    df = classified_df.copy()\n",
    "    feature_cols = get_numeric_features(df)\n",
    "    feature_similarities = []\n",
    "    behavioral_similarities = []\n",
    "    \n",
    "    for _, row in similarity_stats.iterrows():\n",
    "        gen1_data = df[df['generator'] == row['generator1']][feature_cols]\n",
    "        gen2_data = df[df['generator'] == row['generator2']][feature_cols]\n",
    "        \n",
    "        gen1_means = gen1_data.mean(skipna=True)\n",
    "        gen2_means = gen2_data.mean(skipna=True)\n",
    "        gen1_stds = gen1_data.std(skipna=True)\n",
    "        gen2_stds = gen2_data.std(skipna=True)\n",
    "        \n",
    "        valid_features = ~(gen1_means.isna() | gen2_means.isna() | gen1_stds.isna() | gen2_stds.isna())\n",
    "        feature_sim = pd.Series(0.0, index=feature_cols)\n",
    "        feature_sim[valid_features] = exp(-0.1 * abs(gen1_means[valid_features] - gen2_means[valid_features])) * \\\n",
    "                                    exp(-0.1 * abs(gen1_stds[valid_features] - gen2_stds[valid_features]))\n",
    "        \n",
    "        feature_similarities.append(feature_sim)\n",
    "        behavioral_similarities.append(row['mean_similarity'])\n",
    "    \n",
    "    feature_similarities_df = pd.DataFrame(feature_similarities)\n",
    "    correlations = []\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        if feature_similarities_df[feature].isna().any():\n",
    "            continue\n",
    "        correlation = spearmanr(feature_similarities_df[feature], behavioral_similarities, nan_policy='omit')\n",
    "        correlations.append({\n",
    "            'feature': feature,\n",
    "            'correlation': correlation.correlation,\n",
    "            'p_value': correlation.pvalue\n",
    "        })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    corr_df['abs_correlation'] = abs(corr_df['correlation'])\n",
    "    \n",
    "    significant = corr_df[\n",
    "        (corr_df['p_value'] < 0.05) & \n",
    "        (corr_df['abs_correlation'] >= 0.2)\n",
    "    ].sort_values('abs_correlation', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.barh(range(len(significant)), significant['correlation'], \n",
    "            color=plt.cm.viridis(0.5))\n",
    "    plt.yticks(range(len(significant)), significant['feature'], fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.xlabel('Spearman Correlation with Similarity', fontsize=18)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return significant\n",
    "\n",
    "raw_features_df = analyze_instance_similarity_features(classified_df, similarity_matrix, similarity_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
